{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawl IMdB Website for TOP Grossing Movies and their info from each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: selenium in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
            "Requirement already satisfied: trio~=0.17 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: bs4 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\addal\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install selenium\n",
        "%pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.edge.service import Service\n",
        "from selenium.webdriver.edge.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_extract(soup_obj, selector, attribute=None, processing=None):\n",
        "    try:\n",
        "        if isinstance(soup_obj, (int, str)) or soup_obj is None:\n",
        "            return None\n",
        "        element = soup_obj.select_one(selector) if isinstance(selector, str) else soup_obj.find(*selector)\n",
        "        if element:\n",
        "            text = element.get(attribute) if attribute else element.text\n",
        "            return processing(text) if processing else text\n",
        "    except Exception as e:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_awards(awards_text):\n",
        "    \"\"\"\n",
        "    Parse awards text to extract wins, nominations, and oscars.\n",
        "    Returns a dictionary with the counts.\n",
        "    \"\"\"\n",
        "    awards = {\n",
        "        \"wins\": 0,\n",
        "        \"nominations\": 0,\n",
        "        \"oscars\": 0\n",
        "    }\n",
        "    \n",
        "    if not awards_text:\n",
        "        return awards\n",
        "        \n",
        "    try:\n",
        "        # Extract Oscars\n",
        "        oscar_match = awards_text.lower().split('nominated for')\n",
        "        if oscar_match and len(oscar_match) > 1:\n",
        "            oscar_text = oscar_match[1].split()[0]\n",
        "            if oscar_text.isdigit():\n",
        "                awards[\"oscars\"] = int(oscar_text)\n",
        "        \n",
        "        # Extract wins and nominations\n",
        "        if '&' in awards_text:\n",
        "            parts = awards_text.lower().split('&')\n",
        "            \n",
        "            # Extract wins\n",
        "            wins_text = parts[0].split('wins')[0].strip().split()[-1]\n",
        "            if wins_text.isdigit():\n",
        "                awards[\"wins\"] = int(wins_text)\n",
        "            \n",
        "            # Extract nominations\n",
        "            noms_text = parts[1].split('nominations')[0].strip().split()[-1]\n",
        "            if noms_text.isdigit():\n",
        "                awards[\"nominations\"] = int(noms_text)\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing awards: {str(e)}\")\n",
        "        \n",
        "    return awards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_box_office_data(soup, test_id):\n",
        "    try:\n",
        "        element = soup.find(\"li\", {\"data-testid\": f\"title-boxoffice-{test_id}\"})\n",
        "        if element:\n",
        "            content = element.find(\"span\", class_=\"ipc-metadata-list-item__list-content-item\")\n",
        "            if content and content.text:\n",
        "                value = content.text.strip()\n",
        "                # Extract only the number, removing currency symbol and commas\n",
        "                amount = ''.join(c for c in value if c.isdigit())\n",
        "                return int(amount) if amount else None\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_credits(soup):\n",
        "    credits = {\"directors\": [], \"writers\": [], \"stars\": []}\n",
        "\n",
        "    try:\n",
        "        # Find all principal credits\n",
        "        credits_section = soup.find(\"div\", {\"class\": \"sc-70a366cc-2\"})\n",
        "        if not credits_section:\n",
        "            return credits\n",
        "\n",
        "        credit_items = credits_section.find_all(\n",
        "            \"li\", {\"data-testid\": \"title-pc-principal-credit\"}\n",
        "        )\n",
        "\n",
        "        for item in credit_items:\n",
        "            # Get the label (Director/Writers/Stars)\n",
        "            label = item.find(\n",
        "                \"span\", {\"class\": \"ipc-metadata-list-item__label\"}\n",
        "            ) or item.find(\"a\", {\"class\": \"ipc-metadata-list-item__label\"})\n",
        "\n",
        "            if not label:\n",
        "                continue\n",
        "\n",
        "            label_text = label.text.lower().strip()\n",
        "\n",
        "            # Extract names based on the label\n",
        "            names = item.select(\"a.ipc-metadata-list-item__list-content-item--link\")\n",
        "            extracted_names = [name.text.strip() for name in names if name.text.strip()]\n",
        "\n",
        "            if \"director\" in label_text:\n",
        "                credits[\"directors\"] = extracted_names\n",
        "            elif \"writer\" in label_text:\n",
        "                credits[\"writers\"] = extracted_names\n",
        "            elif \"star\" in label_text:\n",
        "                credits[\"stars\"] = extracted_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting credits: {str(e)}\")\n",
        "\n",
        "    return credits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_list_data(soup, selector, class_name=None):\n",
        "    try:\n",
        "        if class_name:\n",
        "            elements = soup.select(f\"{selector} a.{class_name}\")\n",
        "        else:\n",
        "            elements = soup.select(f\"{selector} a.ipc-metadata-list-item__list-content-item--link\")\n",
        "        return [elem.text.strip() for elem in elements if elem.text.strip()]\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_more_items(driver):\n",
        "    try:\n",
        "        # Updated XPath for the \"Load more\" button\n",
        "        button_xpath = \"//button[contains(@class, 'ipc-btn') and contains(@class, 'ipc-see-more')]\"\n",
        "        \n",
        "        # Wait for the button to be clickable\n",
        "        load_more = WebDriverWait(driver, 3).until(\n",
        "            EC.element_to_be_clickable((By.XPATH, button_xpath))\n",
        "        )\n",
        "        \n",
        "        # Scroll to button\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView({ behavior: 'smooth', block: 'center' });\", load_more)\n",
        "        time.sleep(2)\n",
        "        \n",
        "        # Click the button\n",
        "        load_more.click()\n",
        "        time.sleep(2)  # Wait for new content to load\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Load more error: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_genres(soup):\n",
        "    try:\n",
        "        genre_list = soup.select(\"div.ipc-chip-list__scroller a.ipc-chip--on-baseAlt span.ipc-chip__text\")\n",
        "        return [genre.text.strip() for genre in genre_list if genre.text.strip()]\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_imdb_movies(year: int):\n",
        "    output_dir = os.path.join(\"Data\", str(year))\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    url = f\"https://www.imdb.com/search/title/?title_type=feature&release_date={year}-01-01,{year}-12-31&count=10&sort=boxoffice_gross_us,desc\"\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument(\"--lang=en-US\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--window-size=1920,1080\")\n",
        "\n",
        "    driver_path = \"edgedriver.exe\"\n",
        "    service = Service(executable_path=driver_path)\n",
        "    driver = webdriver.Edge(service=service, options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)  # Initial load wait increased\n",
        "\n",
        "        # Wait for the movie list to be present\n",
        "        movie_list_xpath = \"//ul[contains(@class, 'ipc-metadata-list')]\"\n",
        "        WebDriverWait(driver, 3).until(\n",
        "            EC.presence_of_element_located((By.XPATH, movie_list_xpath))\n",
        "        )\n",
        "\n",
        "        # Load more movies\n",
        "        loaded_data = 10\n",
        "        while loaded_data < 20:\n",
        "            print(f\"Current page content length: {len(driver.page_source)}\")\n",
        "            print(f\"Attempting to load more items. Current count: {loaded_data}\")\n",
        "\n",
        "            if load_more_items(driver):\n",
        "                loaded_data += 10\n",
        "                print(f\"Successfully loaded more items. New count: {loaded_data}\")\n",
        "            else:\n",
        "                print(\"Failed to load more items\")\n",
        "                break\n",
        "\n",
        "            # Verify new content is loaded\n",
        "            time.sleep(2)\n",
        "\n",
        "        # Parse the page content\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        # Find the movie list container\n",
        "        movie_list = soup.find(\"ul\", class_=\"ipc-metadata-list\")\n",
        "        if not movie_list:\n",
        "            print(\"Movie list container not found!\")\n",
        "            return None\n",
        "\n",
        "        # Find all movie items\n",
        "        film_containers = movie_list.find_all(\n",
        "            \"li\", class_=\"ipc-metadata-list-summary-item\"\n",
        "        )\n",
        "        print(f\"Found {len(film_containers)} movies for year {year}\")\n",
        "\n",
        "        films_data = []\n",
        "        for film in film_containers:\n",
        "            try:\n",
        "                # Extract title\n",
        "                title_element = film.find(\"h3\", class_=\"ipc-title__text\")\n",
        "                title = title_element.text.strip() if title_element else None\n",
        "\n",
        "                # Extract movie link\n",
        "                link_element = film.find(\"a\", class_=\"ipc-title-link-wrapper\")\n",
        "                movie_link = (\n",
        "                    f\"https://www.imdb.com{link_element['href']}\"\n",
        "                    if link_element and \"href\" in link_element.attrs\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                # Extract metadata\n",
        "                metadata = film.find_all(\"span\", class_=\"dli-title-metadata-item\")\n",
        "                year_text = metadata[0].text if len(metadata) > 0 else None\n",
        "                duration = metadata[1].text if len(metadata) > 1 else None\n",
        "                mpa = metadata[2].text if len(metadata) > 2 else None\n",
        "\n",
        "                # Extract rating\n",
        "                rating_element = film.find(\"span\", class_=\"ipc-rating-star--rating\")\n",
        "                rating = rating_element.text.strip() if rating_element else None\n",
        "\n",
        "                # Extract votes\n",
        "                votes_element = film.find(\"span\", class_=\"ipc-rating-star--voteCount\")\n",
        "                votes = (\n",
        "                    votes_element.text.replace(\" \", \"\").strip(\"()\")[2::]\n",
        "                    if votes_element\n",
        "                    else None\n",
        "                )\n",
        "                film_data = {\n",
        "                    \"Title\": title,\n",
        "                    \"Movie Link\": movie_link,\n",
        "                    \"Year\": year_text,\n",
        "                    \"Duration\": duration,\n",
        "                    \"MPA\": mpa,\n",
        "                    \"Rating\": rating,\n",
        "                    \"Votes\": votes,\n",
        "                }\n",
        "                films_data.append(film_data)\n",
        "                print(f\"Processed movie: {title}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing film: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if not films_data:\n",
        "            print(\"No movies data was collected!\")\n",
        "            return None\n",
        "\n",
        "        # Create DataFrame and save initial data\n",
        "        initial_movies_df = pd.DataFrame(films_data)\n",
        "        initial_movies_path = os.path.join(output_dir, f\"imdb_movies_{year}.csv\")\n",
        "        initial_movies_df.to_csv(initial_movies_path, index=False)\n",
        "\n",
        "        print(f\"Collected {len(initial_movies_df)} movies\")\n",
        "\n",
        "        if (\n",
        "            \"Movie Link\" not in initial_movies_df.columns\n",
        "            or initial_movies_df[\"Movie Link\"].isna().all()\n",
        "        ):\n",
        "            print(f\"Warning: No valid movie links found for year {year}\")\n",
        "            return initial_movies_df\n",
        "\n",
        "        # Extract advanced movie details\n",
        "        all_movie_data = []\n",
        "        for url in initial_movies_df[\"Movie Link\"].dropna():\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                time.sleep(1)\n",
        "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "                # Extract all credits at once\n",
        "                credits = extract_credits(soup)\n",
        "\n",
        "                advanced_details = {\n",
        "                    \"Movie Link\": url,\n",
        "                    \"budget\": extract_box_office_data(soup, \"budget\"),\n",
        "                    \"grossWorldWide\": extract_box_office_data(\n",
        "                        soup, \"cumulativeworldwidegross\"\n",
        "                    ),\n",
        "                    \"gross_US_Canada\": extract_box_office_data(soup, \"grossdomestic\"),\n",
        "                    \"opening_weekend_Gross\": extract_box_office_data(\n",
        "                        soup, \"openingweekenddomestic\"\n",
        "                    ),\n",
        "                    \"directors\": credits[\"directors\"],\n",
        "                    \"writers\": credits[\"writers\"],\n",
        "                    \"stars\": credits[\"stars\"],\n",
        "                    \"genres\": extract_genres(soup),\n",
        "                    \"countries_origin\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-origin']\"\n",
        "                    ),\n",
        "                    \"filming_locations\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-filminglocations']\"\n",
        "                    ),\n",
        "                    \"production_companies\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-companies']\"\n",
        "                    ),\n",
        "                    \"Languages\": extract_list_data(\n",
        "                        soup, \"li[data-testid='title-details-languages']\"\n",
        "                    ),\n",
        "                }\n",
        "\n",
        "                # Extract awards data\n",
        "                awards_element = soup.find(\"li\", {\"data-testid\": \"award_information\"})\n",
        "                if awards_element:\n",
        "                    awards_text = awards_element.text.strip()\n",
        "                    awards_dict = parse_awards(awards_text)\n",
        "                    advanced_details.update(awards_dict)\n",
        "                else:\n",
        "                    advanced_details.update({\"wins\": 0, \"nominations\": 0, \"oscars\": 0})\n",
        "\n",
        "                advanced_details[\"release_date\"] = safe_extract(\n",
        "                    soup, \"a[href*='releaseinfo']\"\n",
        "                )\n",
        "\n",
        "                all_movie_data.append(advanced_details)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "                all_movie_data.append({\"Movie Link\": url})\n",
        "\n",
        "        advanced_movies_df = pd.DataFrame(all_movie_data)\n",
        "        advanced_movies_path = os.path.join(\n",
        "            output_dir, f\"advanced_movies_details_{year}.csv\"\n",
        "        )\n",
        "        advanced_movies_df.to_csv(advanced_movies_path, index=False)\n",
        "\n",
        "        merged_data = pd.merge(\n",
        "            initial_movies_df, advanced_movies_df, how=\"left\", on=\"Movie Link\"\n",
        "        )\n",
        "        merged_path = os.path.join(output_dir, f\"merged_movies_data_{year}.csv\")\n",
        "        merged_data.to_csv(merged_path, index=False)\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    finally:\n",
        "        driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling data for year 2014\n",
            "Current page content length: 874598\n",
            "Attempting to load more items. Current count: 10\n",
            "Successfully loaded more items. New count: 20\n",
            "Found 20 movies for year 2014\n",
            "Processed movie: 1. American Sniper\n",
            "Processed movie: 2. The Hunger Games: Mockingjay - Part 1\n",
            "Processed movie: 3. Guardians of the Galaxy\n",
            "Processed movie: 4. Captain America: The Winter Soldier\n",
            "Processed movie: 5. The Lego Movie\n",
            "Processed movie: 6. The Hobbit: The Battle of the Five Armies\n",
            "Processed movie: 7. Transformers: Age of Extinction\n",
            "Processed movie: 8. Maleficent\n",
            "Processed movie: 9. X-Men: Days of Future Past\n",
            "Processed movie: 10. Big Hero 6\n",
            "Processed movie: 11. Dawn of the Planet of the Apes\n",
            "Processed movie: 12. The Amazing Spider-Man 2\n",
            "Processed movie: 13. Interstellar\n",
            "Processed movie: 14. Godzilla\n",
            "Processed movie: 15. 22 Jump Street\n",
            "Processed movie: 16. Teenage Mutant Ninja Turtles\n",
            "Processed movie: 17. How to Train Your Dragon 2\n",
            "Processed movie: 18. Gone Girl\n",
            "Processed movie: 19. Divergent\n",
            "Processed movie: 20. Neighbors\n",
            "Collected 20 movies\n",
            "Crawling data for year 2015\n",
            "Current page content length: 871448\n",
            "Attempting to load more items. Current count: 10\n",
            "Successfully loaded more items. New count: 20\n",
            "Found 20 movies for year 2015\n",
            "Processed movie: 1. Star Wars: Episode VII - The Force Awakens\n",
            "Processed movie: 2. Jurassic World\n",
            "Processed movie: 3. Avengers: Age of Ultron\n",
            "Processed movie: 4. Inside Out\n",
            "Processed movie: 5. Furious 7\n",
            "Processed movie: 6. Minions\n",
            "Processed movie: 7. The Hunger Games: Mockingjay - Part 2\n",
            "Processed movie: 8. The Martian\n",
            "Processed movie: 9. Cinderella\n",
            "Processed movie: 10. Spectre\n",
            "Processed movie: 11. Mission: Impossible - Rogue Nation\n",
            "Processed movie: 12. Pitch Perfect 2\n",
            "Processed movie: 13. The Revenant\n",
            "Processed movie: 14. Ant-Man\n",
            "Processed movie: 15. Home\n",
            "Processed movie: 16. Hotel Transylvania 2\n",
            "Processed movie: 17. Fifty Shades of Grey\n",
            "Processed movie: 18. The SpongeBob Movie: Sponge Out of Water\n",
            "Processed movie: 19. Straight Outta Compton\n",
            "Processed movie: 20. San Andreas\n",
            "Collected 20 movies\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[196], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years_to_crawl:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrawling data for year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mcrawl_imdb_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[195], line 129\u001b[0m, in \u001b[0;36mcrawl_imdb_movies\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m initial_movies_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMovie Link\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna():\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m         \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    131\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:393\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:382\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    380\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 382\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    137\u001b[0m         method,\n\u001b[0;32m    138\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# years_to_crawl = range(1966, 2014)\n",
        "# for year in years_to_crawl:\n",
        "#     print(f\"Crawling data for year {year}\")\n",
        "#     crawl_imdb_movies(year)\n",
        "# years_to_crawl = range(2019, 2024)\n",
        "# for year in years_to_crawl:\n",
        "#     print(f\"Crawling data for year {year}\")\n",
        "#     crawl_imdb_movies(year)\n",
        "\n",
        "years_to_crawl = range(2014, 2019)\n",
        "for year in years_to_crawl:\n",
        "    print(f\"Crawling data for year {year}\")\n",
        "    crawl_imdb_movies(year)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
