{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crawl IMdB Website for TOP Grossing Movies and their info from each year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install selenium\n",
        "%pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.edge.service import Service\n",
        "from selenium.webdriver.edge.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_extract(soup_obj, selector, attribute=None, processing=None):\n",
        "    try:\n",
        "        if isinstance(soup_obj, (int, str)) or soup_obj is None:\n",
        "            return None\n",
        "        element = soup_obj.select_one(selector) if isinstance(selector, str) else soup_obj.find(*selector)\n",
        "        if element:\n",
        "            text = element.get(attribute) if attribute else element.text\n",
        "            return processing(text) if processing else text\n",
        "    except Exception as e:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_box_office_data(soup, selector_id):\n",
        "    try:\n",
        "        section = soup.find(\"div\", {\"data-testid\": selector_id})\n",
        "        if section:\n",
        "            value = section.select_one(\"div.ipc-metadata-list-item__content-container span\")\n",
        "            return value.text.strip() if value else None\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_credits(soup, credit_type):\n",
        "    try:\n",
        "        credits_section = soup.find(\"div\", {\"data-testid\": \"title-pc-wide-screen\"})\n",
        "        if credits_section:\n",
        "            credit_div = credits_section.find(\"li\", {\"data-testid\": f\"title-pc-principal-credit-{credit_type}\"})\n",
        "            if credit_div:\n",
        "                return [name.text.strip() for name in credit_div.find_all(\"a\")]\n",
        "    except:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_imdb_movies(year: int):\n",
        "    \"\"\"\n",
        "    Crawl IMDb movie data for a specific year\n",
        "\n",
        "    Args:\n",
        "        year (int): Year to crawl movie data for\n",
        "    \"\"\"\n",
        "    # Create output directories\n",
        "    output_dir = os.path.join(\"Data\", str(year))\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Construct URL with dynamic year\n",
        "    url = f\"https://www.imdb.com/search/title/?title_type=feature&release_date={year}-01-01,{year}-12-31&count=100&sort=boxoffice_gross_us,desc\"\n",
        "    # url = f\"https://www.imdb.com/search/title/?title_type=feature&release_date={year}-01-01,{year}-12-31&count=10&sort=boxoffice_gross_us,desc\"\n",
        "\n",
        "    # Configure Edge options\n",
        "    options = Options()\n",
        "    options.add_argument(\"--lang=en-US\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--disable-extensions\")\n",
        "    options.add_argument(\"--disable-browser-side-navigation\")\n",
        "    options.add_argument(\"--disable-infobars\")\n",
        "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "\n",
        "    # Setup WebDriver\n",
        "    driver_path = \"edgedriver.exe\"\n",
        "    service = Service(executable_path=driver_path)\n",
        "    driver = webdriver.Edge(service=service, options=options)\n",
        "\n",
        "    try:\n",
        "        # Navigate to page\n",
        "        driver.get(url)\n",
        "        time.sleep(2)  # Initial page load wait\n",
        "\n",
        "        # Load more movies\n",
        "        loaded_data = 100\n",
        "        # loaded_data = 10\n",
        "        while loaded_data != 600:\n",
        "            try:\n",
        "                # Locate and click \"Load More\" button\n",
        "                load_more_button = WebDriverWait(driver, 5).until(\n",
        "                    EC.presence_of_element_located(\n",
        "                        (\n",
        "                            By.XPATH,\n",
        "                            \"//button[contains(@class, 'ipc-btn') and .//span[contains(text(), '100 more')]]\",\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Scroll and click button\n",
        "                driver.execute_script(\n",
        "                    \"arguments[0].scrollIntoView(true);\", load_more_button\n",
        "                )\n",
        "                driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
        "\n",
        "                time.sleep(3)  # Wait for new data\n",
        "                loaded_data += 100\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"No more 'Load More' button or error: {e}\")\n",
        "                break\n",
        "\n",
        "        # Parse page content\n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        films = soup.find(\n",
        "            \"ul\",\n",
        "            class_=\"ipc-metadata-list ipc-metadata-list--dividers-between sc-748571c8-0 gFCVNT detailed-list-view ipc-metadata-list--base\",\n",
        "        )\n",
        "        films_data = []\n",
        "\n",
        "        # Extract movie metadata\n",
        "        for film in films.find_all(\"li\", class_=\"ipc-metadata-list-summary-item\"):\n",
        "            title = safe_extract(film, [\"h3\", {\"class\": \"ipc-title__text\"}])\n",
        "\n",
        "            metadata_div = film.find(\n",
        "                \"div\", class_=\"sc-300a8231-6 dBUjvq dli-title-metadata\"\n",
        "            )\n",
        "            spans = metadata_div.find_all(\"span\") if metadata_div else []\n",
        "\n",
        "            year_text = spans[0].text if len(spans) > 0 else None\n",
        "            duration = spans[1].text if len(spans) > 1 else None\n",
        "            mpa = spans[2].text if len(spans) > 2 else None\n",
        "\n",
        "            rating = safe_extract(film, [\"span\", {\"class\": \"ipc-rating-star--rating\"}])\n",
        "\n",
        "            link_tag = film.find(\"a\", class_=\"ipc-lockup-overlay ipc-focusable\")\n",
        "            movie_link = f\"https://www.imdb.com{link_tag['href']}\" if link_tag else None\n",
        "\n",
        "            vote_count = safe_extract(\n",
        "                film,\n",
        "                [\"span\", {\"class\": \"ipc-rating-star--voteCount\"}],\n",
        "                processing=lambda x: x.strip().replace(\"\\xa0\", \"\")[1:-1],\n",
        "            )\n",
        "\n",
        "            meta_score = safe_extract(film, [\"span\", {\"class\": \"metacritic-score-box\"}])\n",
        "\n",
        "            description = safe_extract(\n",
        "                film,\n",
        "                [\"div\", {\"class\": \"ipc-html-content-inner-div\"}],\n",
        "                processing=lambda x: x.strip(),\n",
        "            )\n",
        "\n",
        "            films_data.append(\n",
        "                {\n",
        "                    \"Title\": title,\n",
        "                    \"Year\": year_text,\n",
        "                    \"Duration\": duration,\n",
        "                    \"MPA\": mpa,\n",
        "                    \"Rating\": rating,\n",
        "                    \"Votes\": vote_count,\n",
        "                    \"meta_score\": meta_score,\n",
        "                    \"description\": description,\n",
        "                    \"Movie Link\": movie_link,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Save initial movies data\n",
        "        initial_movies_df = pd.DataFrame(films_data)\n",
        "        initial_movies_path = os.path.join(output_dir, f\"imdb_movies_{year}.csv\")\n",
        "        initial_movies_df.to_csv(initial_movies_path, index=False)\n",
        "\n",
        "        # Advanced movie details collection\n",
        "        all_movie_data = []\n",
        "        for url in list(initial_movies_df[\"Movie Link\"]):\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                time.sleep(2)\n",
        "                html = driver.page_source\n",
        "                soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "                advanced_details = {\n",
        "                    \"link\": url,\n",
        "                    # Box Office Data\n",
        "                    \"budget\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"BoxOffice\"}),\n",
        "                        [\"div\", {\"class\": \"ipc-metadata-list-item__content-container\"}],\n",
        "                        processing=lambda x: x.find(\"span\").text.strip(),\n",
        "                    ),\n",
        "                    \"grossWorldWide\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"title-boxoffice-cumulative\"}),\n",
        "                        [\n",
        "                            \"span\",\n",
        "                            {\"class\": \"ipc-metadata-list-item__list-content-item\"},\n",
        "                        ],\n",
        "                        processing=lambda x: x.text.strip(),\n",
        "                    ),\n",
        "                    \"gross_US_Canada\": safe_extract(\n",
        "                        soup.find(\n",
        "                            \"div\", {\"data-testid\": \"title-boxoffice-grossdomestic\"}\n",
        "                        ),\n",
        "                        [\n",
        "                            \"span\",\n",
        "                            {\"class\": \"ipc-metadata-list-item__list-content-item\"},\n",
        "                        ],\n",
        "                        processing=lambda x: x.text.strip(),\n",
        "                    ),\n",
        "                    # Credits\n",
        "                    \"writers\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"title-pc-principal-credits\"}),\n",
        "                        [\"li\", {\"data-testid\": \"title-pc-writer\"}],\n",
        "                        processing=lambda x: [\n",
        "                            writer.text.strip() for writer in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"directors\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"title-pc-principal-credits\"}),\n",
        "                        [\"li\", {\"data-testid\": \"title-pc-director\"}],\n",
        "                        processing=lambda x: [\n",
        "                            director.text.strip() for director in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"stars\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"title-pc-principal-credits\"}),\n",
        "                        [\"li\", {\"data-testid\": \"title-pc-actors\"}],\n",
        "                        processing=lambda x: [\n",
        "                            actor.text.strip() for actor in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    # Metadata\n",
        "                    \"genres\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"genres\"}),\n",
        "                        [\"div\", {\"class\": \"ipc-chip-list\"}],\n",
        "                        processing=lambda x: [\n",
        "                            genre.text.strip()\n",
        "                            for genre in x.find_all(\"span\", class_=\"ipc-chip__text\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"Languages\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"Details\"}),\n",
        "                        [\"div\", {\"data-testid\": \"title-details-languages\"}],\n",
        "                        processing=lambda x: [\n",
        "                            lang.text.strip() for lang in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"countries_origin\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"Details\"}),\n",
        "                        [\"div\", {\"data-testid\": \"title-details-origin\"}],\n",
        "                        processing=lambda x: [\n",
        "                            country.text.strip() for country in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"production_companies\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"Details\"}),\n",
        "                        [\"div\", {\"data-testid\": \"title-details-companies\"}],\n",
        "                        processing=lambda x: [\n",
        "                            company.text.strip() for company in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    \"filming_locations\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"Locations\"}),\n",
        "                        [\"div\", {\"class\": \"ipc-metadata-list-item__content-container\"}],\n",
        "                        processing=lambda x: [\n",
        "                            loc.text.strip() for loc in x.find_all(\"a\")\n",
        "                        ],\n",
        "                    ),\n",
        "                    # Awards\n",
        "                    \"awards\": safe_extract(\n",
        "                        soup.find(\"div\", {\"data-testid\": \"awards\"}),\n",
        "                        [\"div\", {\"class\": \"ipc-metadata-list-item__content-container\"}],\n",
        "                        processing=lambda x: {\n",
        "                            \"wins\": (\n",
        "                                int(re.search(r\"(\\d+)\\s*wins?\", x.text, re.I).group(1))\n",
        "                                if re.search(r\"(\\d+)\\s*wins?\", x.text, re.I)\n",
        "                                else 0\n",
        "                            ),\n",
        "                            \"nominations\": (\n",
        "                                int(\n",
        "                                    re.search(\n",
        "                                        r\"(\\d+)\\s*nominations?\", x.text, re.I\n",
        "                                    ).group(1)\n",
        "                                )\n",
        "                                if re.search(r\"(\\d+)\\s*nominations?\", x.text, re.I)\n",
        "                                else 0\n",
        "                            ),\n",
        "                            \"oscars\": (\n",
        "                                int(\n",
        "                                    re.search(\n",
        "                                        r\"Won\\s*(\\d+)\\s*Oscars?\", x.text, re.I\n",
        "                                    ).group(1)\n",
        "                                )\n",
        "                                if re.search(r\"Won\\s*(\\d+)\\s*Oscars?\", x.text, re.I)\n",
        "                                else 0\n",
        "                            ),\n",
        "                        },\n",
        "                    ),\n",
        "                    \"release_date\": safe_extract(\n",
        "                        soup.find(\"section\", {\"data-testid\": \"Details\"}),\n",
        "                        [\"div\", {\"data-testid\": \"title-details-release-date\"}],\n",
        "                        processing=lambda x: x.find(\"a\").text.split(\" (\")[0].strip(),\n",
        "                    ),\n",
        "                }\n",
        "                all_movie_data.append(advanced_details)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "        # Save advanced movie details\n",
        "        advanced_movies_df = pd.DataFrame(all_movie_data)\n",
        "        advanced_movies_path = os.path.join(\n",
        "            output_dir, f\"advanced_movies_details_{year}.csv\"\n",
        "        )\n",
        "        advanced_movies_df.to_csv(advanced_movies_path, index=False)\n",
        "\n",
        "        # Merge datasets\n",
        "        advanced_movies_df.rename(columns={\"link\": \"Movie Link\"}, inplace=True)\n",
        "        merged_data = pd.merge(\n",
        "            initial_movies_df, advanced_movies_df, how=\"inner\", on=\"Movie Link\"\n",
        "        )\n",
        "        merged_path = os.path.join(output_dir, f\"merged_movies_data_{year}.csv\")\n",
        "        merged_data.to_csv(merged_path, index=False)\n",
        "\n",
        "        return merged_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# years_to_crawl = range(1966, 2014)\n",
        "# for year in years_to_crawl:\n",
        "#     print(f\"Crawling data for year {year}\")\n",
        "#     crawl_imdb_movies(year)\n",
        "# years_to_crawl = range(2019, 2024)\n",
        "# for year in years_to_crawl:\n",
        "#     print(f\"Crawling data for year {year}\")\n",
        "#     crawl_imdb_movies(year)\n",
        "\n",
        "years_to_crawl = range(2014, 2019)\n",
        "for year in years_to_crawl:\n",
        "    print(f\"Crawling data for year {year}\")\n",
        "    crawl_imdb_movies(year)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
